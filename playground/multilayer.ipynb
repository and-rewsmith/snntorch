{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output: tensor([[0.0114]], grad_fn=<AddmmBackward0>)\n",
      "Loss: 0.000130068336147815\n",
      "Gradients for first layer's forward weights: tensor([[ 6.1538e-05,  7.1619e-06, -6.2029e-05, -3.7185e-05, -1.5717e-04,\n",
      "          2.0723e-05,  6.9320e-05,  6.9700e-05, -2.6066e-06,  6.9380e-05],\n",
      "        [-0.0000e+00, -0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "         -0.0000e+00, -0.0000e+00, -0.0000e+00,  0.0000e+00, -0.0000e+00],\n",
      "        [-0.0000e+00, -0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "         -0.0000e+00, -0.0000e+00, -0.0000e+00,  0.0000e+00, -0.0000e+00],\n",
      "        [-0.0000e+00, -0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "         -0.0000e+00, -0.0000e+00, -0.0000e+00,  0.0000e+00, -0.0000e+00],\n",
      "        [-1.5739e-04, -1.8318e-05,  1.5865e-04,  9.5106e-05,  4.0199e-04,\n",
      "         -5.3004e-05, -1.7730e-04, -1.7827e-04,  6.6667e-06, -1.7745e-04],\n",
      "        [-0.0000e+00, -0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "         -0.0000e+00, -0.0000e+00, -0.0000e+00,  0.0000e+00, -0.0000e+00],\n",
      "        [-4.0344e-04, -4.6954e-05,  4.0666e-04,  2.4378e-04,  1.0304e-03,\n",
      "         -1.3586e-04, -4.5446e-04, -4.5696e-04,  1.7089e-05, -4.5485e-04],\n",
      "        [ 5.7654e-05,  6.7099e-06, -5.8114e-05, -3.4838e-05, -1.4725e-04,\n",
      "          1.9415e-05,  6.4945e-05,  6.5302e-05, -2.4421e-06,  6.5001e-05],\n",
      "        [-0.0000e+00, -0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "         -0.0000e+00, -0.0000e+00, -0.0000e+00,  0.0000e+00, -0.0000e+00],\n",
      "        [-0.0000e+00, -0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "         -0.0000e+00, -0.0000e+00, -0.0000e+00,  0.0000e+00, -0.0000e+00]])\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "BATCH_SIZE = 1\n",
    "\n",
    "# Network parameters\n",
    "timesteps = 10\n",
    "layers = 3\n",
    "input_size = 10\n",
    "hidden_size = 10  # Size of each hidden layer\n",
    "output_size = 1   # Output size for each timestep\n",
    "\n",
    "# Define the dense layer class that connects forward and backward\n",
    "class BidirectionalDenseLayer(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size):\n",
    "        super(BidirectionalDenseLayer, self).__init__()\n",
    "        self.backward_layer = nn.Linear(input_size, hidden_size)\n",
    "        self.forward_layer = nn.Linear(hidden_size, hidden_size)\n",
    "        self.activations = torch.zeros(BATCH_SIZE, hidden_size)\n",
    "    \n",
    "    def forward(self, x_forward=None, x_backward=None):\n",
    "        h = torch.zeros(BATCH_SIZE, hidden_size)\n",
    "\n",
    "        if x_forward is not None:\n",
    "            # Forward propagation from the current layer's input\n",
    "            h_forward = self.forward_layer(x_forward)\n",
    "            h += h_forward\n",
    "        \n",
    "        # Backward connection from previous layer (if exists)\n",
    "        if x_backward is not None:\n",
    "            h_backward = self.backward_layer(x_backward)\n",
    "            h += h_backward\n",
    "        \n",
    "        self.activations = torch.relu(h)  # ReLU activation\n",
    "        return self.activations\n",
    "\n",
    "# Define the network with 3 layers and 10 timesteps\n",
    "class BiDirectionalNetwork(nn.Module):\n",
    "    def __init__(self, layers, input_size, hidden_size, output_size):\n",
    "        super(BiDirectionalNetwork, self).__init__()\n",
    "        self.layers = nn.ModuleList([BidirectionalDenseLayer(hidden_size, hidden_size) for _ in range(layers)])\n",
    "        self.output_layer = nn.Linear(hidden_size, output_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Loop over timesteps\n",
    "        for t in range(timesteps):\n",
    "            layer_activations = []\n",
    "            \n",
    "            # Forward pass through each layer\n",
    "            for i, layer in enumerate(self.layers):\n",
    "                if i == 0:\n",
    "                    # First layer does not have a lower layer to connect to\n",
    "                    layer(x, self.layers[i+1].activations)\n",
    "                elif i == len(self.layers) - 1:\n",
    "                    layer(self.layers[i-1].activations, None)\n",
    "                else:\n",
    "                    # Layers above connect to both the previous layer and the current layer's forward pass\n",
    "                    layer(self.layers[i+1].activations, self.layers[i-1].activations)\n",
    "\n",
    "        # Output layer (optional for the last layer at each timestep)\n",
    "        outputs = self.output_layer(self.layers[-1].activations)  # Output from the last layer at the last timestep\n",
    "        \n",
    "        return outputs  # Return both output and activations for later gradient computation\n",
    "\n",
    "# Dummy input to simulate a sequence over 10 timesteps, with batch size = 2\n",
    "x = torch.randn(BATCH_SIZE, input_size)\n",
    "\n",
    "# Initialize the network\n",
    "network = BiDirectionalNetwork(layers, input_size, hidden_size, output_size)\n",
    "\n",
    "# Forward pass\n",
    "output = network(x)\n",
    "\n",
    "# Loss function and backpropagation (global backprop after all timesteps)\n",
    "criterion = nn.MSELoss()\n",
    "target = torch.zeros(1, 1)  # Dummy target for loss computation\n",
    "\n",
    "# Compute loss and backpropagate\n",
    "loss = criterion(output, target)\n",
    "loss.backward()  # This will compute gradients globally, considering all timesteps and layers\n",
    "\n",
    "# Display the output, loss, and gradients\n",
    "print(\"Output:\", output)\n",
    "print(\"Loss:\", loss.item())\n",
    "\n",
    "# Print gradients for the first layer's weights to demonstrate backpropagation\n",
    "print(\"Gradients for first layer's forward weights:\", network.layers[0].forward_layer.weight.grad)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
