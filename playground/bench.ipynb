{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import time\n",
    "import snntorch as snn\n",
    "\n",
    "lif = snn.Leaky(beta=0.9)\n",
    "lif = lif.to('cuda')\n",
    "\n",
    "num_steps = 20000\n",
    "\n",
    "x = torch.rand(num_steps)\n",
    "mem = torch.zeros(1)\n",
    "spk = torch.zeros(1)\n",
    "\n",
    "start_time = time.time()\n",
    "for step in range(num_steps):\n",
    "  spk, mem = lif(x[step], mem=mem)\n",
    "\n",
    "end_time = time.time()\n",
    "\n",
    "print(f\"{end_time-start_time}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lif = snn.LinearLeaky(beta=0.9)\n",
    "lif = lif.to('cuda')\n",
    "\n",
    "# lif = torch.compile(lif, mode=\"reduce-overhead\", fullgraph=True, dynamic=False)\n",
    "\n",
    "timesteps = 100000\n",
    "batch = 1\n",
    "channels = 1\n",
    "print(\"timesteps: \", timesteps)\n",
    "print(\"batch: \", batch)\n",
    "print(\"channels: \", channels)\n",
    "print()\n",
    "\n",
    "input_ = torch.arange(1, timesteps * batch * channels + 1).float().view(timesteps, batch, channels).to('cuda')\n",
    "start_time = time.time()\n",
    "lif.forward(input_)\n",
    "end_time = time.time()\n",
    "\n",
    "print(f\"{end_time-start_time}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import time\n",
    "import snntorch as snn\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "\n",
    "from snntorch._neurons.stateleaky import StateLeaky\n",
    "\n",
    "BATCH_SIZE = 20\n",
    "CHANNELS = 20\n",
    "\n",
    "device = \"cuda\"\n",
    "\n",
    "def bench_type1(num_steps):\n",
    "    lif = snn.Leaky(beta=0.9).to(device)\n",
    "    x = torch.rand(num_steps).to(device)\n",
    "    mem = torch.zeros(BATCH_SIZE, CHANNELS).to(device)\n",
    "    spk = torch.zeros(BATCH_SIZE, CHANNELS).to(device)\n",
    "    \n",
    "    start_time = time.time()\n",
    "    for step in range(num_steps):\n",
    "        spk, mem = lif(x[step], mem=mem)\n",
    "    end_time = time.time()\n",
    "    \n",
    "    return end_time - start_time\n",
    "\n",
    "def bench_type2(timesteps):\n",
    "    lif = StateLeaky(beta=0.9).to(device)\n",
    "    input_ = torch.arange(1, timesteps * BATCH_SIZE * CHANNELS + 1).float().view(timesteps, BATCH_SIZE, CHANNELS).to(device)\n",
    "    \n",
    "    start_time = time.time()\n",
    "    lif.forward(input_)\n",
    "    end_time = time.time()\n",
    "    \n",
    "    return end_time - start_time\n",
    "\n",
    "# Define timesteps on log scale\n",
    "timesteps = np.logspace(1, 5, num=10, dtype=int)\n",
    "\n",
    "# Run benchmarks\n",
    "times1 = []\n",
    "times2 = []\n",
    "for steps in timesteps:\n",
    "    # Run each benchmark multiple times and take average for more stable results\n",
    "    n_runs = 2\n",
    "    time1 = np.mean([bench_type1(steps) for _ in range(n_runs)])\n",
    "    time2 = np.mean([bench_type2(steps) for _ in range(n_runs)])\n",
    "    times1.append(time1)\n",
    "    times2.append(time2)\n",
    "\n",
    "# Create the plot\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(timesteps, times1, 'b-', label='Type 1 (Leaky)')\n",
    "plt.plot(timesteps, times2, 'r-', label='Type 2 (LinearLeaky)')\n",
    "plt.xscale('log')\n",
    "plt.yscale('log')\n",
    "plt.grid(True, which=\"both\", ls=\"-\", alpha=0.2)\n",
    "plt.xlabel('Number of Timesteps')\n",
    "plt.ylabel('Time (seconds)')\n",
    "plt.title('SNN Performance Comparison')\n",
    "plt.legend()\n",
    "plt.savefig(\"snn_performance_comparison.svg\", format=\"svg\")\n",
    "\n",
    "# Print the results\n",
    "print(\"Benchmark Results:\")\n",
    "print(\"\\nTimesteps  |  Leaky (s)  |  Linear Leaky (s)  |  Ratio (T2/T1)\")\n",
    "print(\"-\" * 55)\n",
    "for i, steps in enumerate(timesteps):\n",
    "    ratio = times2[i] / times1[i]\n",
    "    print(f\"{steps:9d} | {times1[i]:10.4f} | {times2[i]:10.4f} | {ratio:10.2f}\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device:  cuda\n",
      "initialized tokenizer\n",
      "tokenized dataset\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Finishing last run (ID:vn6erovj) before initializing another..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">denim-fire-24</strong> at: <a href='https://wandb.ai/and-rewsmith/snntorch-ssm/runs/vn6erovj' target=\"_blank\">https://wandb.ai/and-rewsmith/snntorch-ssm/runs/vn6erovj</a><br/> View project at: <a href='https://wandb.ai/and-rewsmith/snntorch-ssm' target=\"_blank\">https://wandb.ai/and-rewsmith/snntorch-ssm</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20241223_232310-vn6erovj/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Successfully finished last run (ID:vn6erovj). Initializing new run:<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.18.7"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/localuser/Documents/projects/snntorch/playground/wandb/run-20241223_232334-b3qeog51</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/and-rewsmith/snntorch-ssm/runs/b3qeog51' target=\"_blank\">upbeat-music-25</a></strong> to <a href='https://wandb.ai/and-rewsmith/snntorch-ssm' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/and-rewsmith/snntorch-ssm' target=\"_blank\">https://wandb.ai/and-rewsmith/snntorch-ssm</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/and-rewsmith/snntorch-ssm/runs/b3qeog51' target=\"_blank\">https://wandb.ai/and-rewsmith/snntorch-ssm/runs/b3qeog51</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1:   0%|          | 0/16561 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 6.14 GiB. GPU 0 has a total capacity of 23.65 GiB of which 4.48 GiB is free. Including non-PyTorch memory, this process has 19.06 GiB memory in use. Of the allocated memory 14.23 GiB is allocated by PyTorch, and 4.38 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 113\u001b[0m\n\u001b[1;32m    110\u001b[0m x \u001b[38;5;241m=\u001b[39m batch[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minput_ids\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mto(DEVICE)\n\u001b[1;32m    112\u001b[0m \u001b[38;5;66;03m# process batch: one hot / teacher forcing setup / permute to (seq_length, batch, vocab_size)\u001b[39;00m\n\u001b[0;32m--> 113\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mone_hot\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_classes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mVOCAB_SIZE\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mfloat()\n\u001b[1;32m    114\u001b[0m y \u001b[38;5;241m=\u001b[39m x[:, \u001b[38;5;241m1\u001b[39m:]  \u001b[38;5;66;03m# Target: next token in the sequence\u001b[39;00m\n\u001b[1;32m    115\u001b[0m x \u001b[38;5;241m=\u001b[39m x[:, :\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]  \u001b[38;5;66;03m# Input: all but the last token\u001b[39;00m\n",
      "\u001b[0;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 6.14 GiB. GPU 0 has a total capacity of 23.65 GiB of which 4.48 GiB is free. Including non-PyTorch memory, this process has 19.06 GiB memory in use. Of the allocated memory 14.23 GiB is allocated by PyTorch, and 4.38 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "from datasets import load_dataset\n",
    "from transformers import AutoTokenizer\n",
    "from snntorch._neurons.stateleaky import StateLeaky\n",
    "from tqdm import tqdm\n",
    "import torch.nn.functional as F\n",
    "import wandb\n",
    "\n",
    "with open(\"output.txt\", \"w\") as f:\n",
    "    f.write(\"\\n\")\n",
    "\n",
    "def initialize_wandb():\n",
    "    wandb.init(project=\"snntorch-ssm\", config={\n",
    "    })\n",
    "\n",
    "# Hyperparameters\n",
    "SEQ_LENGTH = 128\n",
    "HIDDEN_DIM = 512\n",
    "LR = 1e-3\n",
    "EPOCHS = 10000\n",
    "BATCH_SIZE = 64\n",
    "LEARN_BETA = True\n",
    "\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(\"Device: \", DEVICE)\n",
    "\n",
    "# Load TinyStories dataset from Hugging Face\n",
    "dataset = load_dataset(\"roneneldan/TinyStories\", split=\"train\")\n",
    "\n",
    "\n",
    "\n",
    "# Tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")  # Use GPT-2 tokenizer for simplicity\n",
    "tokenizer.pad_token = tokenizer.eos_token  # Use the end-of-sequence token as padding\n",
    "\n",
    "print(\"initialized tokenizer\")\n",
    "\n",
    "VOCAB_SIZE = tokenizer.vocab_size\n",
    "\n",
    "def tokenize_fn(example):\n",
    "    tokens = tokenizer(example[\"text\"], truncation=True, max_length=SEQ_LENGTH, padding=\"max_length\")\n",
    "    return {\"input_ids\": tokens[\"input_ids\"]}\n",
    "\n",
    "tokenized_dataset = dataset.map(tokenize_fn, batched=True)\n",
    "tokenized_dataset.set_format(type=\"torch\", columns=[\"input_ids\"])\n",
    "\n",
    "print(\"tokenized dataset\")\n",
    "\n",
    "dataloader = DataLoader(tokenized_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "\n",
    "class SNNLanguageModel(nn.Module):\n",
    "    def __init__(self, vocab_size, hidden_dim):\n",
    "        super(SNNLanguageModel, self).__init__()\n",
    "        self.fc1 = nn.Linear(vocab_size, hidden_dim)\n",
    "        self.lif1 = StateLeaky(beta=0.9, channels=hidden_dim, learn_beta=LEARN_BETA)\n",
    "        # self.lif3 = StateLeaky(beta=torch.tensor([0.9]).expand(hidden_dim), channels=hidden_dim, learn_beta=True)\n",
    "        self.fc2 = nn.Linear(hidden_dim, hidden_dim)\n",
    "        self.lif2 = StateLeaky(beta=0.9, channels=hidden_dim, learn_beta=LEARN_BETA)\n",
    "        self.fc3 = nn.Linear(hidden_dim, hidden_dim)\n",
    "        self.lif3 = StateLeaky(beta=0.9, channels=hidden_dim, learn_beta=LEARN_BETA)\n",
    "        self.fc4 = nn.Linear(hidden_dim, vocab_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.reshape(-1, x.shape[-1])\n",
    "\n",
    "        # input transformation\n",
    "        hidden = self.fc1(x)\n",
    "        hidden = hidden.reshape(SEQ_LENGTH-1, -1, hidden.shape[-1])\n",
    "        hidden, _ = self.lif1(hidden)\n",
    "        hidden = hidden.reshape(-1, hidden.shape[-1])\n",
    "\n",
    "        # nonlinear hidden\n",
    "        hidden = self.fc2(hidden)\n",
    "        hidden = torch.relu(hidden)\n",
    "        hidden = hidden.reshape(SEQ_LENGTH-1, -1, hidden.shape[-1])\n",
    "        hidden, _ = self.lif2(hidden)\n",
    "        hidden = hidden.reshape(-1, hidden.shape[-1])\n",
    "\n",
    "        # nonlinear hidden\n",
    "        hidden = self.fc3(hidden)\n",
    "        hidden = torch.relu(hidden)\n",
    "        hidden = hidden.reshape(SEQ_LENGTH-1, -1, hidden.shape[-1])\n",
    "        hidden, _ = self.lif3(hidden)\n",
    "        hidden = hidden.reshape(-1, hidden.shape[-1])\n",
    "\n",
    "        # output transformation\n",
    "        output = self.fc4(hidden)\n",
    "        output = output.reshape(SEQ_LENGTH-1, -1, output.shape[-1])\n",
    "        return output\n",
    "\n",
    "initialize_wandb()\n",
    "\n",
    "# Initialize model, loss, and optimizer\n",
    "model = SNNLanguageModel(VOCAB_SIZE, HIDDEN_DIM).to(DEVICE)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.AdamW(model.parameters(), lr=LR)\n",
    "\n",
    "# Training Loop\n",
    "for epoch in range(EPOCHS):\n",
    "    model.train()\n",
    "    train_loss = 0\n",
    "\n",
    "    batch_num = 0\n",
    "    for batch in tqdm(dataloader, desc=f\"Training Epoch {epoch+1}\"):\n",
    "        batch_num += 1\n",
    "        # print(\"batch num: \", batch_num)\n",
    "        x = batch[\"input_ids\"].to(DEVICE)\n",
    "\n",
    "        # process batch: one hot / teacher forcing setup / permute to (seq_length, batch, vocab_size)\n",
    "        x = F.one_hot(x, num_classes=VOCAB_SIZE).float()\n",
    "        y = x[:, 1:]  # Target: next token in the sequence\n",
    "        x = x[:, :-1]  # Input: all but the last token\n",
    "        x = x.permute(1, 0, 2)\n",
    "        y = y.permute(1, 0, 2)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        output = model(x)\n",
    "\n",
    "        # print the decoding\n",
    "        if batch_num % 50 == 0:\n",
    "            output = output.permute(1, 0, 2)\n",
    "            seq_translate = torch.argmax(output[0], dim=-1)\n",
    "            assert seq_translate.shape[0] == SEQ_LENGTH-1\n",
    "            with open(\"output.txt\", \"a\") as f:\n",
    "                f.write(tokenizer.decode(seq_translate))\n",
    "                f.write(\"\\n\")\n",
    "            output = output.permute(1, 0, 2)\n",
    "\n",
    "        # assert output.shape == (SEQ_LENGTH-1, BATCH_SIZE, VOCAB_SIZE) \n",
    "        # assert y.shape == (SEQ_LENGTH-1, BATCH_SIZE, VOCAB_SIZE)\n",
    "\n",
    "        y = y.argmax(dim=-1)\n",
    "        loss = criterion(output.reshape(-1, VOCAB_SIZE), y.reshape(-1))  # Compute loss\n",
    "        wandb.log({\"loss\": loss.item()})\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        train_loss += loss.item()\n",
    "\n",
    "    train_loss /= len(dataloader)\n",
    "    print(f\"Epoch {epoch+1} Training Loss: {train_loss:.4f}\")\n",
    "\n",
    "print(\"Training Complete!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
